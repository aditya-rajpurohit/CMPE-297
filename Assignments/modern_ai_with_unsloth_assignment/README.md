# ğŸ§  Full Finetuning of SmolLM2-135M on AG News (Topic Classification)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DQPkFwldJZ0LkYIn1xFy_IygrQc-F3cK?usp=sharing)

## ğŸ“‹ Overview
This project demonstrates **full finetuning** of the **SmolLM2-135M** model on the **AG News** dataset using Hugging Face Transformers.   The goal is to train the model to classify short news articles into four topics â€” **World**, **Sports**, **Business**, and **Sci/Tech** â€” using an **instruction-style text format**.

## âš™ï¸ Key Details
- **Model:** [`HuggingFaceTB/SmolLM2-135M`](https://huggingface.co/HuggingFaceTB/SmolLM2-135M)  
- **Dataset:** [`ag_news`](https://huggingface.co/datasets/ag_news)  

## ğŸ§© Notebook Steps
1. **Setup & Installs** â€“ Install lightweight Hugging Face dependencies.  
2. **Imports & Auth** â€“ Load libraries and optionally log in to Hugging Face / W&B.  
3. **Load Model** â€“ Import the SmolLM2 base model and tokenizer.  
4. **Prepare Dataset** â€“ Format AG News into an instruction-style prompt and tokenize.  
5. **Train Model** â€“ Run full fine-tuning using `Trainer`.  
6. **Evaluate & Test** â€“ Generate predictions on unseen news articles.  
7. **Save / Push** â€“ Save the trained model locally or upload to Hugging Face Hub.






# ğŸš€ LoRA Fine-Tuning of SmolLM2-135M on TweetEval Sentiment

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EzYOFZVQMmHWYQ41514CZyTQyZNpsJZg?usp=sharing)

## ğŸ“Œ Overview
This project performs **parameter-efficient LoRA fine-tuning** of **SmolLM2-135M** on **TweetEval/Sentiment** (labels: *Negative*, *Neutral*, *Positive*) using **Unsloth**.  
We load the base model in **4-bit** to save VRAM, attach **LoRA adapters**, and train via **TRLâ€™s SFTTrainer** with an **instruction-style prompt**.

## ğŸ§± Key Components
- **Model**: `unsloth/smollm2-135m`  
- **Dataset**: `tweet_eval` â†’ `sentiment`  
- **Method**: LoRA (`r=16`, `lora_alpha=16`, `lora_dropout=0.05`)  
- **Precision**: 4-bit loading (bitsandbytes) + `bf16/fp16` for training  
- **Trainer**: `trl.SFTTrainer` (supervised finetuning)

## ğŸ”§ Notebook Flow
1. **Installs** â€“ Minimal, pinned to a stable stack for TRL/Transformers + Unsloth.  
2. **Auth** â€“ (Optional) Login to HF Hub + W&B for logging and pushing.  
3. **Load Base Model (4-bit)** â€“ `FastLanguageModel.from_pretrained(...)`.  
4. **Attach LoRA** â€“ `FastLanguageModel.get_peft_model(...)`.  
5. **Dataset & Prompting** â€“ Build instruction-style prompts for Tweetâ†’Label.  
6. **Train** â€“ SFT with `eval_strategy/evaluation_strategy` handled version-safely.  
7. **Inference** â€“ Generate one-word labels (*Negative/Neutral/Positive*).  
8. **Save / Push** â€“ Save LoRA adapters locally or push to HF Hub.




# ğŸ§® DPO Fine-Tuning of SmolLM2-135M on Math Preference Dataset

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_rVwfLe0W8xhjmxW7v5gxiQhL2JHxTAB?usp=sharing)

## ğŸ“˜ Overview
This project demonstrates **Direct Preference Optimization (DPO)** fine-tuning of the **SmolLM2-135M-Instruct** model using **Unsloth** for efficient 4-bit and LoRA-based training.  
The dataset contains small **math reasoning preference pairs** â€” each with a *prompt*, a *preferred (chosen)* solution, and a *rejected* solution.  
The goal is to align the model to prefer **correct mathematical reasoning** and reject incorrect answers.

## âš™ï¸ Key Details
- **Model:** [`unsloth/SmolLM2-135M-Instruct`](https://huggingface.co/unsloth/SmolLM2-135M-Instruct)  
- **Method:** Direct Preference Optimization (DPO)  
- **Techniques:** LoRA (parameter-efficient) + 4-bit quantization (VRAM-efficient)  

## ğŸ§© Notebook Flow
1. **Install & Auth** â€“ Sets up Unsloth, TRL, PEFT, and authenticates Hugging Face + W&B.  
2. **Dataset** â€“ Loads or creates a small math preference dataset (`prompt`, `chosen`, `rejected`).  
3. **Model Setup** â€“ Loads SmolLM2 in 4-bit and attaches LoRA adapters.  
4. **DPO Training** â€“ Runs lightweight preference optimization with Unslothâ€™s patched `DPOTrainer`.  
5. **Inference** â€“ Tests the model on unseen math problems.  
6. **Save / Push** â€“ Saves the fine-tuned model locally or uploads it to the Hugging Face Hub.




